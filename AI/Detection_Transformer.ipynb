{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABA 2025 Jan Hands-On AI\n",
    "\n",
    "Intsructor: Zhun-Gee Ong (Dept. of Data & Knowledge Service Engineering, DKU)\n",
    "\n",
    "## Outline of the day\n",
    "1. Object Detection with HuggingFace and DETR.\n",
    "2. Object Segmentation with SAM.\n",
    "3. PyTorch Tutorial: Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DETR for Object Detection**\n",
    "\n",
    "Paper: https://arxiv.org/abs/2005.12872\n",
    "\n",
    "DETR (DEtection TRansformers) is a deep learning model for object detection and segmentation that uses a Transformer-based architecture. Unlike traditional convolutional object detectors, DETR leverages the attention mechanism from Transformers to directly predict object positions and classes without relying on anchor boxes or complex post-processing like non-maximum suppression (NMS).\n",
    "\n",
    "Key Features:\n",
    "\n",
    "- Transformer-based: It combines a CNN backbone (e.g., ResNet) for feature extraction with a Transformer that captures global context in the image through self-attention.\n",
    "- Set-based Prediction: DETR treats object detection as a direct set prediction problem, using bipartite matching to pair predictions with ground truth objects, simplifying the training process.\n",
    "- End-to-end: Its design allows for end-to-end training and inference, making it simpler and more flexible than many traditional methods, though it may require more computational resources for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DETR](./diagrams/detr.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downside:\n",
    "\n",
    " - High Computational Cost: DETR requires significant computational resources, especially during training, due to its use of Transformers, which rely heavily on self-attention mechanisms. This can make it challenging to train without access to powerful hardware.\n",
    "\n",
    " - Slow Convergence: Compared to traditional convolutional object detectors like Faster R-CNN or YOLO, DETR has a slower training process. It often needs more training epochs to achieve competitive accuracy.\n",
    "\n",
    " - Difficulty with Small Objects: DETR can struggle with detecting small objects in complex scenes. Its attention mechanism, while powerful for capturing global context, might overlook finer details that are essential for detecting smaller objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DETR Results](./diagrams/detr_compare.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR with HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start we always want to makesure the packages that are going to be used later are already installed. Here, we will be installing the **transformers** library from HuggingFace. Just execute the command below in your conda environment:\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\82109\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\transformers\\\\models\\\\deprecated\\\\trajectory_transformer\\\\convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\82109\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.1 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.1 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.1 kB 131.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.1/44.1 kB 216.9 kB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.2.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82109\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.7/57.7 kB ? eta 0:00:00\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\82109\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\82109\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.9/10.1 MB 18.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.5/10.1 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.0/10.1 MB 28.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.9/10.1 MB 34.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.3/10.1 MB 37.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 37.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.1/10.1 MB 38.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 29.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "   ---------------------------------------- 0.0/450.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 450.7/450.7 kB 29.4 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.1-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/12.9 MB 68.9 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.5/12.9 MB 19.7 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.8/12.9 MB 22.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 33.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.2/12.9 MB 35.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.3/12.9 MB 35.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.4/12.9 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 32.8 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/162.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 162.0/162.0 kB 9.5 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 274.1/274.1 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 303.8/303.8 kB 18.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.6/2.4 MB 50.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 37.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 25.3 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 164.9/164.9 kB ? eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl (102 kB)\n",
      "   ---------------------------------------- 0.0/102.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 102.4/102.4 kB ? eta 0:00:00\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "   ---------------------------------------- 0.0/183.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 183.9/183.9 kB 10.9 MB/s eta 0:00:00\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "   ---------------------------------------- 0.0/70.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 70.4/70.4 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 128.4/128.4 kB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection, DetrConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we are going to use a random image as out input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRACTICE => get input image \n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "plt.show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRACTICE => create model\n",
    "backbone = \"facebook/detr-resnet-101\"\n",
    "img_processor = DetrImageProcessor.from_pretrained(\n",
    "    backbone\n",
    ")\n",
    "detr = DetrForObjectDetection.from_pretrained(\n",
    "    backbone\n",
    ")\n",
    "detr = detr.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRACTICE => process input image\n",
    "input_img = img_processor(images=img, return_tensors=\"pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is *pixel_mask*?\n",
    "\n",
    "\n",
    "The pixel_mask is a binary tensor of the same spatial dimensions as the preprocessed image (after resizing). It indicates which pixels in the image are valid (i.e., belong to the actual image) and which are padding pixels.\n",
    "\n",
    "Value of 1: Represents valid pixels (part of the original image).\n",
    "Value of 0: Represents padded pixels (added to create uniform batch sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise pixel_values\n",
    "pv = input_img[\"pixel_values\"].squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "print(pv.shape)\n",
    "\n",
    "plt.imshow(pv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = detr(input_img[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/huggingface/transformers/blob/main/src/transformers/models/detr/modeling_detr.py#L122\n",
    "\n",
    "Component inside the \"outputs\":\n",
    "\n",
    "- loss\n",
    "- loss_dict\n",
    "- logits: torch.FloatTensor = None\n",
    "- pred_boxes: torch.FloatTensor = None\n",
    "- auxiliary_outputs\n",
    "- last_hidden_state\n",
    "- decoder_hidden_states\n",
    "- decoder_attentions\n",
    "- cross_attentions\n",
    "- encoder_last_hidden_state\n",
    "- encoder_hidden_states\n",
    "- encoder_attentions\n",
    "\n",
    "1. logits\n",
    "    - A tensor containing the classificaiton scores for each object query.\n",
    "    - Each row represents a specific object query, adn each column corresponds to a class (plus one column for the \"no object\" class).\n",
    "    - Shape:\n",
    "        \n",
    "        (batch size, num queries, num classes+1)\n",
    "\n",
    "        - num queries: number of object queries (default is 100)\n",
    "        - num classes: number of object categories (91 for COCO dataset)\n",
    "2. pred_boxes\n",
    "    - A tensor containing the predicted bounding boxes for each object query.\n",
    "    - represented in a normalized format:\n",
    "\n",
    "        [x_center, y_center, w, h], where all values are in the range [0, 1].\n",
    "    - Shape:\n",
    "        (batch size, num queries, 4)\n",
    "        - \"4\" represents the bounding box format: len(bbox)=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the logits:\", outputs.logits.shape)\n",
    "print(\"Shape of the pred_boxes:\", outputs.pred_boxes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post_process_object_detection\n",
    "\n",
    "- one of the functions from class *DetrImageProcessor*.\n",
    "- Parameters:\n",
    "    - outputs: Raw outptus of the DETR model.\n",
    "    - target_sizes: (height, weight). Predicitons will not be resize if unset.\n",
    "    - threshold: Score threshold to keep object detection predictions.\n",
    "- Returns:\n",
    "    - A list of dictionaries.\n",
    "    - Each dict: scores, labels, bounding boxes.\n",
    "    - BBox format: (top_left_x, top_left_y, bottom_right_x, bottom_right_y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-processing of outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw bounding boxes on the image.\n",
    "\n",
    "We have two approaches, one is by using PIL library, the other one is utilise CV2 library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes with PIL.\n",
    "\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from random import randrange\n",
    "\n",
    "imgc = img.copy()\n",
    "\n",
    "draw = ImageDraw.Draw(imgc)\n",
    "w = 2 #width of outline\n",
    "font = ImageFont.truetype('FreeMono.ttf', 30)\n",
    "\n",
    "color = [0, 0, 0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    pred_obj = detr.config.id2label[label.item()]\n",
    "\n",
    "    tl = [int(i) for i in box[0:2]] #top_left\n",
    "    br = [int(i) for i in box[2:]] #bottom_right\n",
    "\n",
    "    x1y1x2y2 = (tl[0], tl[1], br[0], br[1])\n",
    "\n",
    "    rc = randrange(len(color))\n",
    "    color[rc] = randrange(255)\n",
    "\n",
    "    # draw bounding box\n",
    "    draw.rectangle(x1y1x2y2, outline=tuple(color), width=w)\n",
    "\n",
    "    # put text\n",
    "    draw.text(tl, str(pred_obj), font=font, fill=tuple(color))\n",
    "\n",
    "    print(\n",
    "        f\"Detected {pred_obj} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "\n",
    "imgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes with CV2.\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgc = np.array(img.copy())\n",
    "# imgc = cv2.cvtColor(np.array(img.copy()), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "thickness=2\n",
    "font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "color = [0, 0, 0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    pred_obj = detr.config.id2label[label.item()]\n",
    "\n",
    "    top_left = [int(i) for i in box[0:2]]\n",
    "    bottom_right = [int(i) for i in box[2:]]\n",
    "\n",
    "    rc = random.randrange(len(color))\n",
    "    color[rc] = 255\n",
    "\n",
    "    imgc = cv2.rectangle(imgc, top_left, bottom_right, color, thickness)\n",
    "    imgc = cv2.putText(imgc, str(pred_obj), top_left, font, 1, color, 2)\n",
    "\n",
    "    print(\n",
    "        f\"Detected {pred_obj} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "\n",
    "plt.imshow(imgc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention of the Last Decoder Layer of DETR\n",
    "\n",
    "This corresponds to visualizing, for each detected object, which part of the image the model was looking at to predict this specific bounding box and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only predictions of queries with 0.9+ confidence (excluding no-object class)\n",
    "probas = outputs.logits.softmax(-1)[0, :, :-1].cpu()\n",
    "keep = probas.max(-1).values > 0.9\n",
    "\n",
    "bboxes_scaled = results['boxes'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lists to store the outputs via up-values\n",
    "conv_features = []\n",
    "\n",
    "hooks = [\n",
    "    detr.model.backbone.conv_encoder.register_forward_hook(\n",
    "        lambda self, input, output: conv_features.append(output)\n",
    "    ),\n",
    "]\n",
    "\n",
    "# propagate through the model\n",
    "outputs = detr(**input_img, output_attentions=True)\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# don't need the list anymore\n",
    "conv_features = conv_features[0]\n",
    "# get cross-attention weights of last decoder layer - which is of shape (batch_size, num_heads, num_queries, width*height)\n",
    "dec_attn_weights = outputs.cross_attentions[-1].cpu()\n",
    "# average them over the 8 heads and detach from graph\n",
    "dec_attn_weights = torch.mean(dec_attn_weights, dim=1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature map shape\n",
    "h, w = conv_features[-1][0].shape[-2:]\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=2, figsize=(22, 7))\n",
    "colors = COLORS * 100\n",
    "for idx, ax_i, box in zip(keep.nonzero(), axs.T, bboxes_scaled):\n",
    "    xmin, ymin, xmax, ymax = box.cpu().detach().numpy()\n",
    "    ax = ax_i[0]\n",
    "    ax.imshow(dec_attn_weights[0, idx].view(h, w))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'query id: {idx.item()}')\n",
    "    ax = ax_i[1]\n",
    "    ax.imshow(img)\n",
    "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                               fill=False, color='blue', linewidth=3))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(detr.config.id2label[probas[idx].argmax().item()])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segmentation Anything Model (SAM)**\n",
    "\n",
    "[Link to paper](https://arxiv.org/abs/2304.02643)\n",
    "\n",
    "[SAM Playground](https://segment-anything.com/)\n",
    "\n",
    "[Code reference](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb)\n",
    "\n",
    "Segment Anythin Model, a.k.a. SAM, is a highly versatile and general-purpose image segmentation model developed by Meta AI. It is designed to \"segment anything\" in images with minimal user input. The model can identify objects, parts, or regions in an image, enabling a wide range of applications.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "- **Generalization**:\n",
    "    - SAM is trained on a massive dataset of over 1 billion masks.\n",
    "    - It can segment objects in unseen images and even new domains with minimal fine-tuning.\n",
    "- **Interactive Segmentation**:\n",
    "    - Users can guide the model by providing points, bounding boxes, or free-form input to indicate the regions of interest.\n",
    "- **Multi-modal Input**:\n",
    "    - It supports text prompts, which means you can describe the object you want to segment.\n",
    "- **Multiple Outputs**:\n",
    "    - SAM generates multiple segmentation masks for ambiguous scenarios, offering flexibility to choose the best result.\n",
    "- **Fast and Efficient**:\n",
    "    - Real-time inference is possible, making it suitable for tasks requiring quick segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Segment Anything Model (SAM)](./diagrams/sam_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAM relies on a ViT (Vision Transformer) architecture, which excels in understanding visual data. The model integrates:\n",
    "    \n",
    "- Encoder: Extracts rich features from the image.\n",
    "- Prompt Encoder: Processes user inputs (e.g., points, boxes).\n",
    "- Mask Decoder: Generates segmentation masks based on the prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM from Meta\n",
    "\n",
    "First, install the necessary libraries:\n",
    "\n",
    "```\n",
    "pip install torch torchvision\n",
    "pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "pip install opencv-python matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install SAM library from Meta\n",
    "! pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "\n",
    "# Download model checkpoint from the official Github\n",
    "! wget -P ./sam_weight https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth \n",
    "! wget -P ./sam_weight https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "! wget -P ./sam_weight https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "\n",
    "# Download example images (Option)\n",
    "!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
    "!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Objects with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SAM model can be loaded with 3 different encoders: ViT-B, ViT-L, and ViT-H.\n",
    "'''\n",
    "model_type = \"vit_b\"  # Options: vit_b, vit_l, vit_h\n",
    "\n",
    "checkpoints_root = \"./sam_weight\"\n",
    "sam_vit_b = os.path.join(checkpoints_root, \"sam_vit_b_01ec64.pth\")\n",
    "sam_vit_l = os.path.join(checkpoints_root, \"sam_vit_l_0b3195.pth\")\n",
    "sam_vit_h = os.path.join(checkpoints_root, \"sam_vit_h_4b8939.pth\") # default\n",
    "\n",
    "if model_type == \"vit_b\":\n",
    "    sam_checkpoint = os.path.join(checkpoints_root, \"sam_vit_b_01ec64.pth\")\n",
    "elif model_type == \"vit_l\":\n",
    "    sam_checkpoint = os.path.join(checkpoints_root, \"sam_vit_l_0b3195.pth\")\n",
    "else:\n",
    "    print(\"Using vit_h\")\n",
    "    model_type = \"vit_h\"\n",
    "    sam_checkpoint = os.path.join(checkpoints_root, \"sam_vit_h_4b8939.pth\")\n",
    "\n",
    "# PRACTICE Load the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our SAM look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the image to produce an image embedding by calling `SamPredictor.set_image`. SamPredictor remembers this embedding and will use it for subsequent mask prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image of predictor\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the truck, choose a point on it. Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point). Multiple points can be input; here we use only one. The chosen point will be shown as a star on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375]])\n",
    "input_label = np.array([1])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with ``SamPredictor.predict``. The model returns masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRACTICE make prediction with predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With ``multimask_output=True`` (the default setting), SAM outputs 3 masks, where scores gives the model's own estimation of the quality of these masks.\n",
    "- This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt.\n",
    "- When False, it will return a single mask. \n",
    "- For ambiguous prompts such as a single point, it is recommended to use multimask_output=True even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in scores. This will often result in a better mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape  # (number_of_masks) x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    show_mask(mask, plt.gca())\n",
    "    show_points(input_point, input_label, plt.gca())\n",
    "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a specific object with additional points\n",
    "\n",
    "- The single input point is ambiguous, and the model has returned multiple objects consistent with it.\n",
    "- To obtain a single object, multiple points can be provided. If available, a mask from a previous iteration can also be supplied to the model to aid in prediction.\n",
    "- When specifying a single object with multiple prompts, a single mask can be requested by setting ``multimask_output=False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_by_points(img, masks, points, labels):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img)\n",
    "    show_mask(masks, plt.gca())\n",
    "    show_points(points, labels, plt.gca())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# we can make all operations with one line\n",
    "def predict_masks(points, labels, scores, multi_mask=True):\n",
    "    mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n",
    "    \n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=points,\n",
    "        point_labels=labels,\n",
    "        mask_input=mask_input[None, :, :],\n",
    "        multimask_output=multi_mask,\n",
    "    )\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375], [1125, 625]])\n",
    "input_label = np.array([1, 1])\n",
    "\n",
    "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = predict_masks(input_point, input_label, scores, multi_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_by_points(image, masks, input_point, input_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To exclude the car and specify just the window, a background point (with label 0, here shown in red) can be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_label = np.array([1, 0])\n",
    "\n",
    "masks = predict_masks(input_point, input_label, scores, multi_mask=False)\n",
    "\n",
    "vis_by_points(image, masks, input_point, input_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a specific object with a box\n",
    "\n",
    "The model can also take a box as input, provided in xyxy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = np.array([425, 600, 700, 875])\n",
    "# right rear wheel: [425, 600, 700, 875]\n",
    "\n",
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining points and boxes\n",
    "\n",
    "Points and boxes may be combined, just by including both types of prompts to the predictor. Here this can be used to select just the trucks's tire, instead of the entire wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = np.array([425, 600, 700, 875])\n",
    "input_point = np.array([[575, 750]])\n",
    "input_label = np.array([0])\n",
    "\n",
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    box=input_box,\n",
    "    multimask_output=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Custom Dataset\n",
    "\n",
    "1. Custom Dataset\n",
    "2. Data Loader\n",
    "\n",
    "## Custom Dataset\n",
    "\n",
    "When dealing with deep learning projects, data usually doesn't come in a perfect format. You may have images stored in a specific directory structure, or you might have data stored in CSV files.\n",
    "\n",
    "In these case, writing a custom dataset class helps load and process data in a structured way that PyTorch can work with.\n",
    "\n",
    "Creating custom dataset in PyTorch cna be done by subclassing the **Dataset** class from <mark>**torch.utils.data**</mark>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_folder, label_folder, transform=None):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_folder (string/path): Path of the folder where the data is stored.\n",
    "            data_folder (string/path): Path of the folder where the annotation file is stored.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        self.db = self._load_annotation()\n",
    "\n",
    "    def _load_annotation(self):\n",
    "\n",
    "        \"\"\"\n",
    "        In this function, we can define how our annotation file will be read and process.\n",
    "        \"\"\"\n",
    "\n",
    "        processed_annotation = None\n",
    "\n",
    "        return processed_annotation\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample, target = self.db[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiate our **CustomDataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "my_dataset = CustomDataset(data_folder=\"./\", label_folder=\"./\", transform=tfms)\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset=my_dataset,\n",
    "#     batch_size=64,\n",
    "#     shuffle=True,       # normally True for training set, False for test and validation set\n",
    "#     sampler=None,       # index of sample that will be used, normally assigned when doing k-fold validation\n",
    "#     num_workers=2,      # number of subprocesses to use for data loading\n",
    "#     pin_memory=True     #set to True when using GPU training\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a cats vs dogs dataset with following directory tree:\n",
    "```\n",
    "${root_of_the_dataset}\n",
    " |--anno\n",
    " |  |--train_anno.json\n",
    " |  `--val_anno.json\n",
    " `--images\n",
    "    |--train\n",
    "    |  |-- class_0\n",
    "    |  |   |-- 01.jpg\n",
    "    |  |   |-- 02.jpg\n",
    "    |  |   |-- 03.jpg\n",
    "    |  |   |-- ...\n",
    "    |  |-- class_1\n",
    "    |  |   |-- 01.jpg\n",
    "    |  |   |-- 02.jpg\n",
    "    |  |   |-- 03.jpg\n",
    "    |  |   |-- ...\n",
    "    `--validation\n",
    "       |-- class_0\n",
    "       |   |-- 01.jpg\n",
    "       |   |-- 02.jpg\n",
    "       |   |-- 03.jpg\n",
    "       |   |-- ...\n",
    "       |-- class_1\n",
    "       |    |-- 01.jpg\n",
    "       |    |-- 02.jpg\n",
    "       |    |-- 03.jpg\n",
    "       |    |-- ...\n",
    "```\n",
    "\n",
    "(dataset can be downloaded from: )\n",
    "\n",
    "We can create the custom dataset class for this cats vs dogs dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRACTICE\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CatDog(Dataset):\n",
    "    def __init__(self, root, one_hot_enc=False, transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # HAPPY CODING\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # HAPPY CODING\n",
    "\n",
    "        return \n",
    "    \n",
    "    def _load_anno(self):\n",
    "\n",
    "        # HAPPY CODING\n",
    "\n",
    "        return \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # HAPPY CODING\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PRACTICE\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "        # HAPPY CODING\n",
    "\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_ENC = True\n",
    "\n",
    "# HAPPY CODING\n",
    "\n",
    "# train_ds = CatDog()\n",
    "\n",
    "\n",
    "# val_ds = CatDog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# HAPPY CODING\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    # HAPPY CODING\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    # HAPPY CODING\n",
    "\n",
    ")\n",
    "\n",
    "for img, target in train_loader:\n",
    "    print(\"Input image shape: {}\".format(img.shape))\n",
    "    print(\"Target shape: {}\".format(target.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
